import streamlit as st
import requests
from bs4 import BeautifulSoup
from PIL import Image
import re

try:
    from duckduckgo_search import DDGS
except ImportError:
    st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ duckduckgo-search –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    DDGS = None

st.set_page_config(page_title="–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–æ–≤–∏–∫", layout="wide")

class InternetResearchAI:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_duckduckgo(self, query: str, num_results: int = 5):
        if DDGS is None:
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return []
            
        try:
            with DDGS() as ddgs:
                results = []
                for result in ddgs.text(query, max_results=num_results):
                    results.append({
                        'title': result.get('title', ''),
                        'url': result.get('href', ''),
                        'description': result.get('body', '')
                    })
                return results
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_page_content(self, url: str):
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            text = soup.get_text()
            text = re.sub(r'\s+', ' ', text)
            return text[:2000]
        except:
            return ""
    
    def extract_text_from_image(self, image):
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ OCR
            width, height = image.size
            return f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {width}x{height} –ø–∏–∫—Å–µ–ª–µ–π. –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫."
        except:
            return "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"

def main():
    st.title("üß† –£–º–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–æ–≤–∏–∫")
    st.write("–ò—â–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    if DDGS is None:
        st.warning("–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫ –ø–æ–∏—Å–∫–∞")
    
    research_ai = InternetResearchAI()
    
    option = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –ø–æ–∏—Å–∫–∞:", 
                     ["üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫", "üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"])
    
    if option == "üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫":
        st.header("–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫")
        text_query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:", 
                                 placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç 2024")
        
        if st.button("üîç –ù–∞–π—Ç–∏") and text_query:
            with st.spinner("–ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é..."):
                results = research_ai.search_duckduckgo(text_query, 5)
                
                if results:
                    st.success(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    for i, result in enumerate(results, 1):
                        with st.expander(f"{i}. {result['title']}"):
                            st.write(f"**–°—Å—ã–ª–∫–∞:** {result['url']}")
                            st.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {result['description']}")
                            
                            content = research_ai.get_page_content(result['url'])
                            if content:
                                st.write("**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:**", content[:500] + "...")
                else:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    
    else:
        st.header("–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        st.info("–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
        
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 
                                       type=['png', 'jpg', 'jpeg'])
        
        if uploaded_file is not None:
            try:
                image = Image.open(uploaded_file)
                st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)
                
                if st.button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"):
                    with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ..."):
                        image_info = research_ai.extract_text_from_image(image)
                        
                        st.subheader("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏:")
                        st.write(image_info)
                        
                        st.info("–î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ")
                        
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

if __name__ == "__main__":
    main()